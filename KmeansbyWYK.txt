//simple implementation for Kmeans in scala by Yuankai Wang
// -------------------------------------------------------------------------------------------------------------------

//to start coding you need these webpages as references
// -------------------------------------------------------------------------------------------------------------------
//http://stackoverflow.com/questions/33866759/spark-scala-dataframe-create-feature-vectors
//http://stackoverflow.com/questions/31447141/spark-mllib-kmeans-from-dataframe-and-back-again
//https://spark.apache.org/docs/1.6.1/ml-clustering.html
//https://spark.apache.org/docs/1.6.0/ml-features.html#vectorassembler


//these code should run in spark shell
//spark 1.6 can not read csv so load the package
// -------------------------------------------------------------------------------------------------------------------
spark-shell --packages com.databricks:spark-csv_2.11:1.1.0


//import library for Mlib
// -------------------------------------------------------------------------------------------------------------------
import org.apache.spark._
import org.apache.spark.sql._
import org.apache.spark.sql.types._
import org.apache.spark.sql.functions._
import org.apache.spark.ml.clustering.KMeans


//the csv doesn't have schema, so create schema for it
//this structureType is for the yellow taxi data in NYC
// ------------------------------------------------------------------------------------------------------------------- 
val yellowSchema = StructType(Array(
  StructField("VendorID", IntegerType, true),
  StructField("tpep_pickup_datetime", StringType, true),
  StructField("tpep_dropoff_datetime", StringType, true),
  StructField("Passenger_count", IntegerType, true),
  StructField("Trip_distance", DoubleType, true),
  StructField("Pickup_longitude", DoubleType, true),
  StructField("Pickup_latitude", DoubleType, true),
  StructField("RateCodeID", IntegerType, true),
  StructField("Store_and_fwd_flag", StringType, true),
  StructField("Dropoff_longitude", DoubleType, true),
  StructField("Dropoff_latitude", DoubleType, true),
  StructField("Payment_type", IntegerType, true),
  StructField("Fare_amount", DoubleType, true),
  StructField("Extra", DoubleType, true),
  StructField("MTA_tax", DoubleType, true),
  StructField("Improvement_surcharge", DoubleType, true),
  StructField("Tip_amount", DoubleType, true),
  StructField("Tolls_amount", DoubleType, true),
  StructField("Total_amount", DoubleType, true)
))

//load data from hdfs, you shold change path
val taxiDF = sqlContext.read.format("com.databricks.spark.csv").option("header", "false").schema(yellowSchema).load("/user/yw2504/finalProject/yellow/2016/yellow_tripdata_2016-04.csv")


//the input of Kmeans trainer is feature vector, the following code is used to generate feature vector
// -------------------------------------------------------------------------------------------------------------------

//select data from input, this sample is for clustering the pickup station 
taxiDF.registerTempTable("trip")
val DF = sqlContext.sql("Select tpep_pickup_datetime, Pickup_latitude, Pickup_longitude From trip")

//import the libraries for vector
import org.apache.spark.ml.feature.VectorAssembler
import org.apache.spark.mllib.linalg.Vectors
val assembler = new VectorAssembler().setInputCols(Array("Pickup_latitude", "Pickup_longitude")).setOutputCol("features")
//the feature vector is needed to cache in memory
val featureVector = assembler.transform(DF).select("features").cache()


//create the buffer writer using java package
// -------------------------------------------------------------------------------------------------------------------
import java.io._
val file = new File("/home/yw2504/project/cost")
val bw = new BufferedWriter(new FileWriter(file))
bw.write("this the output of computeCost")

//train 99 Kmeans models to choose the better k
//the output of computerCost are writed to the file 
// -------------------------------------------------------------------------------------------------------------------
val clusterNum:Array[Int] = 2 to 100 toArray
clusterNum.foreach(cluster => {
 val kmeans = new KMeans().setK(cluster).setFeaturesCol("features").setPredictionCol("prediction").setMaxIter(30)
 val model = kmeans.fit(featureVector)
 val ssd = model.computeCost(featureVector)
 bw.write(cluster + "," + ssd)
 bw.newLine();
})

bw.close()





